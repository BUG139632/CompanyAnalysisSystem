# 爬虫Agent规则（Crawler Agent Rules）

## 适用范围
- 巨潮资讯网、东方财富网、同花顺财经、证券交易所官网、上市公司官网
- 目标数据：财务报表、公告、专利数据、新产品发布信息、研发投入、销售模式

---

## 1. 合规性与道德规范
- 必须遵守目标网站robots.txt协议，禁止抓取被明令禁止的内容。
- 请求频率需合理，避免对目标网站造成压力（建议每个域名间隔1-3秒）。
- 禁止抓取涉及个人隐私或违反法律法规的数据。
- 必须设置明确的User-Agent标识。

## 2. 数据格式规范
- 爬虫输出数据需为如下结构（Python dict）：

```python
{
    "url": str,              # 数据来源页面URL
    "source": str,           # 网站名称，如"巨潮资讯网"
    "crawl_time": str,       # 抓取时间，格式YYYY-MM-DD HH:MM:SS
    "data_type": str,        # 数据类型，如"公告"、"财务报表"等
    "raw_content": str,      # 原始HTML或主要文本
    "title": str,            # 页面或公告标题（可选）
    "extra": dict            # 针对不同网站/数据类型的特殊字段（可选）
}
```

- 所有字段必须有，除title、extra可为空。
- 时间格式统一为`YYYY-MM-DD HH:MM:SS`。

## 3. 技术实现规则
- 支持断点续爬，避免重复抓取。
- 对网络异常、超时等情况应有重试机制，最多重试3次。
- 支持代理池配置，防止单一IP被封。
- 日志需详细记录抓取进度、异常和关键事件。

## 4. 反爬虫与反检测
- User-Agent需模拟主流浏览器，可定期更换。
- 支持必要的Cookie、Header伪装。
- 对动态渲染页面，需支持Selenium/Playwright等工具。
- 如遇验证码，需记录并跳过该页面。

## 5. 数据完整性与溯源
- 必须保存原始页面内容（raw_content），便于后续数据清洗和溯源。
- 每条数据需包含抓取时间和来源URL。

## 6. 其他
- 遵循模块化、可扩展的代码结构，便于后续维护和扩展。
- 所有配置（如目标URL、请求头、代理等）应支持配置文件或参数化。 