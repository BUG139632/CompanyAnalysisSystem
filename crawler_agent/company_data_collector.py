#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…¬å¸æ•°æ®é‡‡é›†å™¨
ç”¨äºé‡‡é›†æŒ‡å®šå…¬å¸çš„è´¢æŠ¥ã€å…¬å‘Šå’Œè¡Œä¸šç ”æŠ¥
"""
import json
import os
import shutil
from datetime import datetime
from typing import Dict, List, Optional

# å¯¼å…¥æ•°æ®æºæ¨¡å—
from crawler_agent.data_source.eastmoney_data_source import (
    fetch_eastmoney_announcements,
    fetch_eastmoney_annual_reports,
    fetch_eastmoney_industry_reports_by_company,
    get_fresh_eastmoney_session,
    EASTMONEY_INDUSTRY_LIST,
    gemini_llm_func
)
from crawler_agent.data_source.cninfo_data_source import (
    fetch_cninfo_financial_reports
)
from crawler_agent.data_source.szse_data_source import (
    fetch_szse_announcements
)
from crawler_agent.data_source.thsl_data_source import (
    fetch_thsl_financial_reports
)
from crawler_agent.crawl4ai_agent_improved import ImprovedCrawl4AIAgent
from common.llm_base_agent import LLMBaseAgent
import asyncio
# å¯¼å…¥å®˜ç½‘é‡‡é›†å¤ç”¨å‡½æ•°
from crawler_agent.data_source.website_data_source import (
    search_company_website,
    find_investor_page,
    extract_report_links,
    download_file
)

def get_company_code_by_llm(company_name: str) -> str:
    """
    ä½¿ç”¨LLMæ ¹æ®å…¬å¸åç§°è·å–è‚¡ç¥¨ä»£ç 
    """
    prompt = f"""
    è¯·æ ¹æ®å…¬å¸åç§°"{company_name}"è¿”å›å…¶è‚¡ç¥¨ä»£ç ã€‚
    åªè¿”å›è‚¡ç¥¨ä»£ç ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
    ä¾‹å¦‚ï¼š
    è´µå·èŒ…å° -> 600519
    å¹³å®‰é“¶è¡Œ -> 000001
    è…¾è®¯æ§è‚¡ -> 00700
    é˜¿é‡Œå·´å·´ -> 09988
    """
    print(f"[LLM PROMPT] {prompt}")
    
    try:
        # ä½¿ç”¨çœŸå®çš„LLMæ¥å£
        agent = LLMBaseAgent()
        result = agent.llm_generate(prompt)
        if result:
            return result.strip()
    except Exception as e:
        print(f"[è­¦å‘Š] LLMè°ƒç”¨å¤±è´¥: {e}")

class CompanyDataCollector:
    """å…¬å¸æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self, output_dir: str = "data/raw"):
        self.output_dir = output_dir
        self.setup_output_dirs()
        
    def setup_output_dirs(self):
        """è®¾ç½®è¾“å‡ºç›®å½•ç»“æ„"""
        dirs = [
            "announcements",
            "financial_reports", 
            "industry_reports",
            "announcements/eastmoney_announcements",
            "announcements/szse_announcements",
            "announcements/cninfo_announcements",
            "financial_reports/eastmoney_financial_reports",
            "financial_reports/szse_financial_reports", 
            "financial_reports/cninfo_financial_reports",
            "financial_reports/thsl_financial_reports",
            "industry_reports/eastmoney"
        ]
        for dir_path in dirs:
            full_path = os.path.join(self.output_dir, dir_path)
            os.makedirs(full_path, exist_ok=True)
    
    def clear_historical_data(self, company_code: str = None):
        """
        æ¸…ç©ºå†å²æ•°æ®
        :param company_code: å¦‚æœæŒ‡å®šå…¬å¸ä»£ç ï¼Œåªæ¸…ç©ºè¯¥å…¬å¸çš„æ•°æ®ï¼›å¦åˆ™æ¸…ç©ºæ‰€æœ‰å†å²æ•°æ®
        """
        print(f"ğŸ§¹ å¼€å§‹æ¸…ç©ºå†å²æ•°æ®...")
        
        # éœ€è¦æ¸…ç©ºçš„ç›®å½•
        raw_dirs = [
            "announcements/eastmoney_announcements",
            "announcements/szse_announcements", 
            "announcements/cninfo_announcements",
            "financial_reports/eastmoney_financial_reports",
            "financial_reports/szse_financial_reports",
            "financial_reports/cninfo_financial_reports", 
            "financial_reports/thsl_financial_reports",
            "industry_reports/eastmoney"
        ]
        # ä¿®æ­£æ‹¼æ¥æ–¹å¼ï¼Œç›´æ¥åŸºäº self.output_dir æ‹¼æ¥
        dirs_to_clear = [os.path.join(self.output_dir, d) for d in raw_dirs]
        
        total_cleared = 0
        for full_path in dirs_to_clear:
            if os.path.exists(full_path):
                if company_code:
                    # åªæ¸…ç©ºæŒ‡å®šå…¬å¸çš„æ•°æ®
                    for filename in os.listdir(full_path):
                        if filename.startswith(company_code):
                            file_path = os.path.join(full_path, filename)
                            try:
                                os.remove(file_path)
                                total_cleared += 1
                                print(f"   - åˆ é™¤: {file_path}")
                            except Exception as e:
                                print(f"   - åˆ é™¤å¤±è´¥: {file_path}, é”™è¯¯: {e}")
                else:
                    # æ¸…ç©ºæ•´ä¸ªç›®å½•
                    try:
                        shutil.rmtree(full_path)
                        os.makedirs(full_path, exist_ok=True)
                        print(f"   - æ¸…ç©ºç›®å½•: {full_path}")
                    except Exception as e:
                        print(f"   - æ¸…ç©ºç›®å½•å¤±è´¥: {full_path}, é”™è¯¯: {e}")
        
        # æ¸…ç©ºæ±‡æ€»æ–‡ä»¶
        if company_code:
            # åˆ é™¤æŒ‡å®šå…¬å¸çš„æ±‡æ€»æ–‡ä»¶
            for filename in os.listdir(self.output_dir):
                if filename.startswith(company_code) and filename.endswith('.json'):
                    file_path = os.path.join(self.output_dir, filename)
                    try:
                        os.remove(file_path)
                        total_cleared += 1
                        print(f"   - åˆ é™¤æ±‡æ€»æ–‡ä»¶: {file_path}")
                    except Exception as e:
                        print(f"   - åˆ é™¤æ±‡æ€»æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        else:
            # åˆ é™¤æ‰€æœ‰æ±‡æ€»æ–‡ä»¶
            for filename in os.listdir(self.output_dir):
                if filename.endswith('_å…¬å¸æ•°æ®æ±‡æ€»_') and filename.endswith('.json'):
                    file_path = os.path.join(self.output_dir, filename)
                    try:
                        os.remove(file_path)
                        total_cleared += 1
                        print(f"   - åˆ é™¤æ±‡æ€»æ–‡ä»¶: {file_path}")
                    except Exception as e:
                        print(f"   - åˆ é™¤æ±‡æ€»æ–‡ä»¶å¤±è´¥: {file_path}, é”™è¯¯: {e}")
        
        print(f"âœ… å†å²æ•°æ®æ¸…ç©ºå®Œæˆï¼Œå…±åˆ é™¤ {total_cleared} ä¸ªæ–‡ä»¶")
    
    def collect_company_data(self, company_name: str):
        """
        é‡‡é›†æŒ‡å®šå…¬å¸çš„æ‰€æœ‰æ•°æ®
        :param company_name: å…¬å¸åç§°
        :return: é‡‡é›†ç»“æœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹é‡‡é›†å…¬å¸æ•°æ®: {company_name}")
        print(f"{'='*60}")
        
        # ä½¿ç”¨LLMè·å–è‚¡ç¥¨ä»£ç 
        print("æ­£åœ¨è·å–è‚¡ç¥¨ä»£ç ...")
        company_code = get_company_code_by_llm(company_name)
        print(f"è·å–åˆ°è‚¡ç¥¨ä»£ç : {company_code}")
        
        # æ¸…ç©ºæ‰€æœ‰å†å²æ•°æ®
        self.clear_historical_data()
        
        # è·å–ä¸œæ–¹è´¢å¯Œsession
        print("æ­£åœ¨è·å–ä¸œæ–¹è´¢å¯Œsession...")
        cookies, headers = get_fresh_eastmoney_session()
        
        results = {
            "company_name": company_name,
            "company_code": company_code,
            "collect_time": datetime.now().isoformat(),
            "data": {}
        }
        
        # 1. é‡‡é›†å…¬å¸å…¬å‘Š
        print(f"\nğŸ“¢ å¼€å§‹é‡‡é›†å…¬å¸å…¬å‘Š...")
        announcement_results = self.collect_announcements(company_name, company_code, cookies, headers)
        results["data"]["announcements"] = announcement_results
        
        # 2. é‡‡é›†è´¢åŠ¡æŠ¥è¡¨
        print(f"\nğŸ“Š å¼€å§‹é‡‡é›†è´¢åŠ¡æŠ¥è¡¨...")
        financial_results = self.collect_financial_reports(company_name, company_code, cookies, headers)
        results["data"]["financial_reports"] = financial_results
        
        # 3. é‡‡é›†è¡Œä¸šç ”æŠ¥
        print(f"\nğŸ“ˆ å¼€å§‹é‡‡é›†è¡Œä¸šç ”æŠ¥...")
        industry_results = self.collect_industry_reports(company_name, cookies, headers)
        results["data"]["industry_reports"] = industry_results

        # ã€å…¬å¸å®˜ç½‘é‡‡é›†ç¤ºä¾‹ - å¯ç”¨LLMè¾…åŠ©å…³é”®è¯ç”Ÿæˆã€‘
        print(f"\nğŸŒ å°è¯•é‡‡é›†å…¬å¸å®˜ç½‘è´¢æŠ¥/å…¬å‘Š...")
        website_url = search_company_website(company_name, llm_func=gemini_llm_func)
        if website_url:
            investor_page = find_investor_page(website_url)
            if investor_page:
                links = extract_report_links(investor_page)
                for link in links:
                    download_file(link, save_dir="data/raw/announcements/website_announcements")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        print(f"\nâœ… å…¬å¸ {company_name} æ•°æ®é‡‡é›†å®Œæˆï¼")
        return results
    
    def collect_announcements(self, company_name: str, company_code: str, cookies: dict, headers: dict) -> dict:
        """é‡‡é›†å…¬å¸å…¬å‘Š"""
        results = {"sources": {}}
        
        try:
            # ä¸œæ–¹è´¢å¯Œå…¬å‘Š
            print("  - é‡‡é›†ä¸œæ–¹è´¢å¯Œå…¬å‘Š...")
            eastmoney_data = fetch_eastmoney_announcements(
                company_code=company_code,
                page_size=30,
                page_index=1,
                save=True,
                save_dir=f"{self.output_dir}/announcements/eastmoney_announcements",
                cookies=cookies,
                headers=headers
            )
            results["sources"]["eastmoney"] = {
                "status": "success",
                "count": len(eastmoney_data.get('data', [])),
                "data": eastmoney_data
            }
        except Exception as e:
            print(f"  - ä¸œæ–¹è´¢å¯Œå…¬å‘Šé‡‡é›†å¤±è´¥: {e}")
            results["sources"]["eastmoney"] = {"status": "failed", "error": str(e)}
        
        try:
            # æ·±äº¤æ‰€å…¬å‘Š
            print("  - é‡‡é›†æ·±äº¤æ‰€å…¬å‘Š...")
            szse_data = fetch_szse_announcements(
                company_code=company_code,
                page_size=30,
                save=True,
                save_dir=f"{self.output_dir}/announcements/szse_announcements"
            )
            results["sources"]["szse"] = {
                "status": "success", 
                "count": len(szse_data.get('data', [])),
                "data": szse_data
            }
        except Exception as e:
            print(f"  - æ·±äº¤æ‰€å…¬å‘Šé‡‡é›†å¤±è´¥: {e}")
            results["sources"]["szse"] = {"status": "failed", "error": str(e)}
        
        return results
    
    def collect_financial_reports(self, company_name: str, company_code: str, cookies: dict, headers: dict) -> dict:
        """é‡‡é›†è´¢åŠ¡æŠ¥è¡¨"""
        results = {"sources": {}}
        
        try:
            # ä¸œæ–¹è´¢å¯Œè´¢æŠ¥
            print("  - é‡‡é›†ä¸œæ–¹è´¢å¯Œè´¢æŠ¥...")
            eastmoney_data = fetch_eastmoney_annual_reports(company_code)
            if (
                isinstance(eastmoney_data, dict)
                and isinstance(eastmoney_data.get('result'), dict)
                and isinstance(eastmoney_data['result'].get('data'), list)
                and eastmoney_data['result']['data']
            ):
                results["sources"]["eastmoney"] = {
                    "status": "success",
                    "count": len(eastmoney_data['result']['data']),
                    "data": eastmoney_data
                }
                print(f"  - ä¸œæ–¹è´¢å¯Œè´¢æŠ¥é‡‡é›†æˆåŠŸ: {len(eastmoney_data['result']['data'])}æ¡è®°å½•")
            else:
                results["sources"]["eastmoney"] = {
                    "status": "no_data",
                    "message": "æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®",
                    "data": eastmoney_data
                }
                print("  - ä¸œæ–¹è´¢å¯Œè´¢æŠ¥æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
        except Exception as e:
            print(f"  - ä¸œæ–¹è´¢å¯Œè´¢æŠ¥é‡‡é›†å¤±è´¥: {e}")
            results["sources"]["eastmoney"] = {"status": "failed", "error": str(e)}
        
        try:
            # æ·±äº¤æ‰€è´¢æŠ¥
            print("  - é‡‡é›†æ·±äº¤æ‰€è´¢æŠ¥...")
            szse_data = fetch_szse_announcements(
                company_code=company_code,
                download_pdfs=True,
                max_pdfs=5,
                datatype='è´¢æŠ¥'
            )
            if isinstance(szse_data, dict) and szse_data.get('data'):
                results["sources"]["szse"] = {
                    "status": "success",
                    "count": len(szse_data.get('data', [])),
                    "data": szse_data
                }
                print(f"  - æ·±äº¤æ‰€è´¢æŠ¥é‡‡é›†æˆåŠŸ: {len(szse_data.get('data', []))}æ¡è®°å½•")
            else:
                results["sources"]["szse"] = {
                    "status": "no_data",
                    "message": "æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®",
                    "data": szse_data
                }
                print("  - æ·±äº¤æ‰€è´¢æŠ¥æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
        except Exception as e:
            print(f"  - æ·±äº¤æ‰€è´¢æŠ¥é‡‡é›†å¤±è´¥: {e}")
            results["sources"]["szse"] = {"status": "failed", "error": str(e)}
        
        try:
            # å·¨æ½®èµ„è®¯ç½‘è´¢æŠ¥ - ä½¿ç”¨æ”¹è¿›çš„Crawl4AIä»£ç†
            print("  - é‡‡é›†å·¨æ½®èµ„è®¯ç½‘è´¢æŠ¥...")
            cninfo_data = self._collect_cninfo_financial_reports(company_name, company_code)
            if cninfo_data and len(cninfo_data) > 0:
                results["sources"]["cninfo"] = {
                    "status": "success",
                    "count": len(cninfo_data),
                    "data": cninfo_data
                }
                print(f"  - å·¨æ½®èµ„è®¯ç½‘è´¢æŠ¥é‡‡é›†æˆåŠŸ: {len(cninfo_data)}ä»½æŠ¥å‘Š")
            else:
                results["sources"]["cninfo"] = {
                    "status": "no_data",
                    "message": "æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®",
                    "data": cninfo_data
                }
                print("  - å·¨æ½®èµ„è®¯ç½‘è´¢æŠ¥æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
        except Exception as e:
            print(f"  - å·¨æ½®èµ„è®¯ç½‘è´¢æŠ¥é‡‡é›†å¤±è´¥: {e}")
            results["sources"]["cninfo"] = {"status": "failed", "error": str(e)}
        
        try:
            # åŒèŠ±é¡ºè´¢æŠ¥
            print("  - é‡‡é›†åŒèŠ±é¡ºè´¢æŠ¥...")
            thsl_data = fetch_thsl_financial_reports(company_code)
            if isinstance(thsl_data, dict) and thsl_data:
                results["sources"]["thsl"] = {
                    "status": "success",
                    "count": len(thsl_data.get('data', [])) if isinstance(thsl_data, dict) else 0,
                    "data": thsl_data
                }
                print(f"  - åŒèŠ±é¡ºè´¢æŠ¥é‡‡é›†æˆåŠŸ: {len(thsl_data.get('data', [])) if isinstance(thsl_data, dict) else 0}æ¡è®°å½•")
            else:
                results["sources"]["thsl"] = {
                    "status": "no_data",
                    "message": "æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®",
                    "data": thsl_data
                }
                print("  - åŒèŠ±é¡ºè´¢æŠ¥æœªè·å–åˆ°æœ‰æ•ˆæ•°æ®")
        except Exception as e:
            print(f"  - åŒèŠ±é¡ºè´¢æŠ¥é‡‡é›†å¤±è´¥: {e}")
            results["sources"]["thsl"] = {"status": "failed", "error": str(e)}
        
        return results
    
    def _collect_cninfo_financial_reports(self, company_name: str, company_code: str) -> List[Dict]:
        """
        ä½¿ç”¨æ”¹è¿›çš„Crawl4AIä»£ç†é‡‡é›†å·¨æ½®èµ„è®¯ç½‘è´¢æŠ¥
        :param company_name: å…¬å¸åç§°
        :param company_code: å…¬å¸ä»£ç 
        :return: è´¢æŠ¥æ•°æ®åˆ—è¡¨
        """
        try:
            # åˆ›å»ºå¼‚æ­¥äº‹ä»¶å¾ªç¯
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            async def _async_collect():
                async with ImprovedCrawl4AIAgent() as agent:
                    # ä½¿ç”¨å·¨æ½®èµ„è®¯ç½‘ä¸“ç”¨æ–¹æ³•
                    results = await agent.crawl_cninfo_financial_reports(
                        company_name=company_name,
                        company_code=company_code,
                        max_reports=5
                    )
                    return results
            
            # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
            results = loop.run_until_complete(_async_collect())
            print(f"[DEBUG] å·¨æ½®è¿”å›: {results}")
            loop.close()

            # ä¿å­˜æ¯ä»½æŠ¥å‘Šåˆ° output/financial_reports/cninfo_financial_reports/
            import os, json, datetime
            save_dir = os.path.join('data/raw', 'financial_reports', 'cninfo_financial_reports')
            os.makedirs(save_dir, exist_ok=True)
            now_str = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            for report in results:
                report_type = report.get('report_type', 'unknown')
                fname = f"{company_name}_{company_code}_è´¢åŠ¡æŠ¥è¡¨_{report_type}_{now_str}.json"
                fpath = os.path.join(save_dir, fname)
                with open(fpath, 'w', encoding='utf-8') as f:
                    json.dump(report, f, ensure_ascii=False, indent=2)
                print(f"[å·¨æ½®] å·²ä¿å­˜: {fpath}")

            return results
            
        except Exception as e:
            print(f"    - å·¨æ½®èµ„è®¯ç½‘è´¢æŠ¥é‡‡é›†å¼‚å¸¸: {e}")
            return []
    
    def collect_industry_reports(self, company_name: str, cookies: dict, headers: dict) -> dict:
        """é‡‡é›†è¡Œä¸šç ”æŠ¥"""
        results = {"sources": {}}
        
        try:
            # ä¸œæ–¹è´¢å¯Œè¡Œä¸šç ”æŠ¥
            print("  - é‡‡é›†ä¸œæ–¹è´¢å¯Œè¡Œä¸šç ”æŠ¥...")
            eastmoney_data = fetch_eastmoney_industry_reports_by_company(
                company_name=company_name,
                llm_func=gemini_llm_func,
                page_num=1,
                page_size=30,
                save=True,
                save_dir=f"{self.output_dir}/industry_reports/eastmoney",
                cookies=cookies,
                headers=headers,
                max_pdfs=20
            )
            results["sources"]["eastmoney"] = {
                "status": "success",
                "count": len(eastmoney_data.get('data', [])),
                "data": eastmoney_data
            }
        except Exception as e:
            print(f"  - ä¸œæ–¹è´¢å¯Œè¡Œä¸šç ”æŠ¥é‡‡é›†å¤±è´¥: {e}")
            results["sources"]["eastmoney"] = {"status": "failed", "error": str(e)}
        
        return results
    
    def save_summary_results(self, company_name: str, results: dict):
        """ä¿å­˜æ±‡æ€»ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{company_name}_å…¬å¸æ•°æ®æ±‡æ€»_{timestamp}.json"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(results)
    
    def print_statistics(self, results: dict):
        """æ‰“å°é‡‡é›†ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\nğŸ“Š é‡‡é›†ç»Ÿè®¡:")
        print(f"   å…¬å¸åç§°: {results['company_name']}")
        print(f"   é‡‡é›†æ—¶é—´: {results['collect_time']}")
        
        data = results.get('data', {})
        
        # å…¬å‘Šç»Ÿè®¡
        if 'announcements' in data:
            announcement_stats = data['announcements']['sources']
            total_announcements = sum(
                source.get('count', 0) 
                for source in announcement_stats.values() 
                if source.get('status') == 'success'
            )
            print(f"   å…¬å‘Šæ€»æ•°: {total_announcements}")
        
        # è´¢æŠ¥ç»Ÿè®¡
        if 'financial_reports' in data:
            financial_stats = data['financial_reports']['sources']
            total_financial = sum(
                source.get('count', 0)
                for source in financial_stats.values()
                if source.get('status') == 'success'
            )
            print(f"   è´¢æŠ¥æ€»æ•°: {total_financial}")
        
        # ç ”æŠ¥ç»Ÿè®¡
        if 'industry_reports' in data:
            industry_stats = data['industry_reports']['sources']
            total_industry = sum(
                source.get('count', 0)
                for source in industry_stats.values()
                if source.get('status') == 'success'
            )
            print(f"   ç ”æŠ¥æ€»æ•°: {total_industry}")

def collect_single_company_data(company_name: str, output_dir: str = "data/raw"):
    """
    é‡‡é›†å•ä¸ªå…¬å¸çš„æ‰€æœ‰æ•°æ®
    :param company_name: å…¬å¸åç§°
    :param output_dir: è¾“å‡ºç›®å½•
    :return: é‡‡é›†ç»“æœ
    """
    collector = CompanyDataCollector(output_dir)
    return collector.collect_company_data(company_name)

def collect_multiple_companies_data(companies: List[str], output_dir: str = "data/raw"):
    """
    æ‰¹é‡é‡‡é›†å¤šä¸ªå…¬å¸çš„æ•°æ®
    :param companies: å…¬å¸åç§°åˆ—è¡¨
    :param output_dir: è¾“å‡ºç›®å½•
    :return: æ‰€æœ‰å…¬å¸çš„é‡‡é›†ç»“æœ
    """
    collector = CompanyDataCollector(output_dir)
    all_results = []
    
    for company_name in companies:
        try:
            result = collector.collect_company_data(company_name)
            all_results.append(result)
        except Exception as e:
            print(f"âŒ é‡‡é›†å…¬å¸ {company_name} æ•°æ®æ—¶å‡ºé”™: {e}")
            continue
    
    return all_results

def clear_all_historical_data(output_dir: str = "output"):
    """
    æ¸…ç©ºæ‰€æœ‰å†å²æ•°æ®
    :param output_dir: è¾“å‡ºç›®å½•
    """
    collector = CompanyDataCollector(output_dir)
    collector.clear_historical_data()

def clear_company_historical_data(company_code: str, output_dir: str = "output"):
    """
    æ¸…ç©ºæŒ‡å®šå…¬å¸çš„å†å²æ•°æ®
    :param company_code: å…¬å¸ä»£ç 
    :param output_dir: è¾“å‡ºç›®å½•
    """
    collector = CompanyDataCollector(output_dir)
    collector.clear_historical_data(company_code) 