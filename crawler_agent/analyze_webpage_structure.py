#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æç½‘é¡µç»“æ„ï¼Œæ‰¾å‡ºæ­£ç¡®çš„CSSé€‰æ‹©å™¨
å¸®åŠ©é…ç½®ç½‘é¡µå‹æ•°æ®æºçš„æå–è§„åˆ™
"""

import asyncio
import yaml
from crawl4ai_agent_improved import ImprovedCrawl4AIAgent
from crawl4ai import AsyncWebCrawler

async def analyze_webpage_structure():
    """åˆ†æç½‘é¡µç»“æ„"""
    # print("=== åˆ†æç½‘é¡µç»“æ„ï¼Œæ‰¾å‡ºæ­£ç¡®çš„CSSé€‰æ‹©å™¨ ===")
    
    # æµ‹è¯•URLåˆ—è¡¨
    test_urls = [
        {
            "name": "ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€å®˜ç½‘",
            "url": "http://www.sse.com.cn/disclosure/listedinfo/announcement/c/new/000001.htm"
        },
        {
            "name": "æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€å®˜ç½‘", 
            "url": "http://www.szse.cn/disclosure/listed/fixed/index.html?code=000001"
        },
        {
            "name": "ä¸œæ–¹è´¢å¯Œç½‘",
            "url": "https://basic.10jqka.com.cn/000001/finance.html"
        }
    ]
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = AsyncWebCrawler(verbose=True, headless=True)
    
    try:
        for i, test_info in enumerate(test_urls, 1):
            # print(f"\n--- åˆ†æ {i}: {test_info['name']} ---")
            # print(f"URL: {test_info['url']}")
            
            try:
                # è·å–ç½‘é¡µå†…å®¹
                result = await crawler.arun(url=test_info['url'])
                
                if result and hasattr(result, 'html'):
                    html = result.html
                    # print(f"âœ… ç½‘é¡µè·å–æˆåŠŸï¼Œé•¿åº¦: {len(html)} å­—ç¬¦")
                    
                    # åˆ†æHTMLç»“æ„
                    await analyze_html_structure(html, test_info['name'])
                else:
                    # print(f"âŒ æ— æ³•è·å–ç½‘é¡µå†…å®¹")
                    pass # Removed print statement
                    
            except Exception as e:
                # print(f"âŒ åˆ†æå¤±è´¥: {e}")
                pass # Removed print statement
        
        # å…³é—­çˆ¬è™«
        await crawler.close()
        # print("\nâœ… çˆ¬è™«å·²å…³é—­")
        
    except Exception as e:
        # print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # print("\n=== åˆ†æå®Œæˆ ===")

async def analyze_html_structure(html: str, source_name: str):
    """åˆ†æHTMLç»“æ„ï¼Œæ‰¾å‡ºå¯èƒ½çš„é€‰æ‹©å™¨"""
    # print(f"\nğŸ“Š {source_name} HTMLç»“æ„åˆ†æ:")
    
    # æŸ¥æ‰¾å¸¸è§çš„æ ‡é¢˜é€‰æ‹©å™¨
    title_selectors = [
        'h1', 'h2', 'h3', '.title', '.headline', '.page-title', 
        '.main-title', '.content-title', '.article-title'
    ]
    
    # æŸ¥æ‰¾å¸¸è§çš„å†…å®¹é€‰æ‹©å™¨
    content_selectors = [
        '.content', '.article', '.text', '.main-content', '.body',
        '.description', '.summary', '.detail', '.info'
    ]
    
    # æŸ¥æ‰¾å¸¸è§çš„æ—¥æœŸé€‰æ‹©å™¨
    date_selectors = [
        '.date', '.time', '.publish-time', '.publish-date', 
        '.update-time', '.timestamp', '.datetime'
    ]
    
    # æŸ¥æ‰¾å¸¸è§çš„è¡¨æ ¼é€‰æ‹©å™¨ï¼ˆè´¢åŠ¡æŠ¥è¡¨ç›¸å…³ï¼‰
    table_selectors = [
        'table', '.table', '.data-table', '.financial-table',
        '.report-table', '.statement-table'
    ]
    
    # print("\nğŸ” æ ‡é¢˜é€‰æ‹©å™¨åˆ†æ:")
    for selector in title_selectors:
        count = html.count(f'<{selector}') + html.count(f'class="{selector}"') + html.count(f'class=\'{selector}\'')
        if count > 0:
            # print(f"  {selector}: {count} ä¸ª")
            pass # Removed print statement
    
    # print("\nğŸ” å†…å®¹é€‰æ‹©å™¨åˆ†æ:")
    for selector in content_selectors:
        count = html.count(f'<{selector}') + html.count(f'class="{selector}"') + html.count(f'class=\'{selector}\'')
        if count > 0:
            # print(f"  {selector}: {count} ä¸ª")
            pass # Removed print statement
    
    # print("\nğŸ” æ—¥æœŸé€‰æ‹©å™¨åˆ†æ:")
    for selector in date_selectors:
        count = html.count(f'<{selector}') + html.count(f'class="{selector}"') + html.count(f'class=\'{selector}\'')
        if count > 0:
            # print(f"  {selector}: {count} ä¸ª")
            pass # Removed print statement
    
    # print("\nğŸ” è¡¨æ ¼é€‰æ‹©å™¨åˆ†æ:")
    for selector in table_selectors:
        count = html.count(f'<{selector}') + html.count(f'class="{selector}"') + html.count(f'class=\'{selector}\'')
        if count > 0:
            # print(f"  {selector}: {count} ä¸ª")
            pass # Removed print statement
    
    # æŸ¥æ‰¾æ‰€æœ‰classå±æ€§
    import re
    class_pattern = r'class=["\']([^"\']+)["\']'
    classes = re.findall(class_pattern, html)
    
    # ç»Ÿè®¡æœ€å¸¸è§çš„class
    class_count = {}
    for class_list in classes:
        for class_name in class_list.split():
            class_count[class_name] = class_count.get(class_name, 0) + 1
    
    # æ˜¾ç¤ºæœ€å¸¸è§çš„class
    # print(f"\nğŸ” æœ€å¸¸è§çš„class (å‰10ä¸ª):")
    sorted_classes = sorted(class_count.items(), key=lambda x: x[1], reverse=True)
    for class_name, count in sorted_classes[:10]:
        # print(f"  {class_name}: {count} ä¸ª")
        pass # Removed print statement

if __name__ == "__main__":
    asyncio.run(analyze_webpage_structure()) 